{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas requests\n",
        "!pip install safetensors\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwmanunGWiIK",
        "outputId": "ae6511d1-f13c-465e-86d0-7c87ddb320b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from safetensors.torch import load_file  # Import for loading .safetensors files\n",
        "from google.colab import files  # Remove or modify if not using Colab\n",
        "import os\n",
        "\n",
        "###############################################################################\n",
        "#                              MODEL CONFIGURATIONS                           #\n",
        "###############################################################################\n",
        "MODEL_CONFIGS = {\n",
        "    \"prajjwal1-bert-tiny\": {\n",
        "        \"base_id\": \"prajjwal1/bert-tiny\",\n",
        "        \"finetuned_path\": \"/content/prajjwal1_bert_tiny_model.safetensors\",  # Update as per your path\n",
        "        \"finetuned_type\": \"safetensors\",\n",
        "        \"untuned_label_map\": {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},  # Assumed Mapping\n",
        "        \"finetuned_label_map\": {0: \"Neutral\", 1: \"Bullish\", 2: \"Bearish\"}\n",
        "    },\n",
        "    \"cardiffnlp-twitter-roberta-base\": {\n",
        "        \"base_id\": \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
        "        \"finetuned_path\": \"/content/roberta_model.pt\",  # Update as per your path\n",
        "        \"finetuned_type\": \"pt\",\n",
        "        \"untuned_label_map\": {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
        "        \"finetuned_label_map\": {0: \"Neutral\", 1: \"Bullish\", 2: \"Bearish\"}\n",
        "    },\n",
        "    \"Jedida-distilbert\": {\n",
        "        \"base_id\": \"Jedida/tweet_sentiments_analysis_distilbert\",\n",
        "        \"finetuned_path\": \"/content/Jedida_tweet_sentiments_analysis_distilbert_model.safetensors\",  # Update as per your path\n",
        "        \"finetuned_type\": \"safetensors\",\n",
        "        \"untuned_label_map\": {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"},\n",
        "        \"finetuned_label_map\": {0: \"Neutral\", 1: \"Bullish\", 2: \"Bearish\"}\n",
        "    }\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "#                           LOADING MODELS & TOKENIZERS                       #\n",
        "###############################################################################\n",
        "def load_model_and_tokenizer(\n",
        "    base_id: str,\n",
        "    finetuned_path: str = None,\n",
        "    finetuned_type: str = None,\n",
        "    use_finetuned: bool = False,\n",
        "    device: str = \"cpu\",\n",
        "    num_labels: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads either the untuned version of a model or its fine-tuned version,\n",
        "    depending on `use_finetuned`. Handles .pt vs. .safetensors differences.\n",
        "\n",
        "    **Important:** Always uses the original tokenizer from `base_id`.\n",
        "\n",
        "    :param base_id: Model hub ID for the base pretrained model.\n",
        "    :param finetuned_path: Local path to the fine-tuned weights.\n",
        "    :param finetuned_type: \"pt\" or \"safetensors\" (needed for the loading method).\n",
        "    :param use_finetuned: If True, load the fine-tuned weights. Otherwise, untuned.\n",
        "    :param device: \"cpu\" or \"cuda\".\n",
        "    :param num_labels: Number of sentiment classes (defaults to 3).\n",
        "    :return: (tokenizer, model)\n",
        "    \"\"\"\n",
        "    # Always load the tokenizer from the base model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_id)\n",
        "\n",
        "    if not use_finetuned or not finetuned_path:\n",
        "        # Load untuned model directly from Hugging Face\n",
        "        print(f\"Loading untuned model from {base_id}\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            base_id, num_labels=num_labels\n",
        "        )\n",
        "    else:\n",
        "        # Load fine-tuned weights\n",
        "        print(f\"Loading fine-tuned model from {finetuned_path}\")\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            base_id, num_labels=num_labels\n",
        "        )\n",
        "        if finetuned_type == \"pt\":\n",
        "            # Load the saved state_dict using torch.load\n",
        "            state_dict = torch.load(finetuned_path, map_location=torch.device(device))\n",
        "            model.load_state_dict(state_dict)\n",
        "        elif finetuned_type == \"safetensors\":\n",
        "            # Load the saved state_dict using safetensors\n",
        "            state_dict = load_file(finetuned_path)\n",
        "            model.load_state_dict(state_dict)\n",
        "        else:\n",
        "            raise ValueError(\"finetuned_type must be either 'pt' or 'safetensors'\")\n",
        "\n",
        "    model.to(device)\n",
        "    return tokenizer, model\n",
        "\n",
        "###############################################################################\n",
        "#                             PREDICTION FUNCTION                             #\n",
        "###############################################################################\n",
        "def predict_sentiment(text, tokenizer, model, label_map, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Predict the sentiment for a single text using the given tokenizer and model.\n",
        "    Returns the predicted sentiment label as a string based on the provided label_map.\n",
        "\n",
        "    :param text: The input text for sentiment analysis.\n",
        "    :param tokenizer: Tokenizer associated with the model.\n",
        "    :param model: The sequence classification model.\n",
        "    :param label_map: Dictionary mapping label IDs to sentiment strings.\n",
        "    :param device: Device to perform computation on (\"cpu\" or \"cuda\").\n",
        "    :return: Predicted sentiment label as a string.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    pred_label_id = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return label_map.get(pred_label_id, \"unknown\")\n",
        "\n",
        "###############################################################################\n",
        "#                        FETCHING STOCKTWITS MESSAGES                         #\n",
        "###############################################################################\n",
        "def fetch_stocktwits(symbol, count=10):\n",
        "    \"\"\"\n",
        "    Fetch recent messages from StockTwits API for a specific stock symbol.\n",
        "\n",
        "    :param symbol: Stock symbol, e.g. \"AAPL\"\n",
        "    :param count: Number of messages to fetch\n",
        "    :return: List of dictionaries (id, text, created_at)\n",
        "    \"\"\"\n",
        "    url = f\"https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "        data = response.json()\n",
        "        messages = data.get(\"messages\", [])\n",
        "        return [\n",
        "            {\n",
        "                \"id\": msg[\"id\"],\n",
        "                \"text\": msg[\"body\"],\n",
        "                \"created_at\": msg[\"created_at\"]\n",
        "            }\n",
        "            for msg in messages[:count]\n",
        "        ]\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")  # e.g., 404 Not Found\n",
        "    except Exception as err:\n",
        "        print(f\"An error occurred: {err}\")\n",
        "    return []\n",
        "\n",
        "###############################################################################\n",
        "#                                    MAIN                                     #\n",
        "###############################################################################\n",
        "def main():\n",
        "    # Parameters\n",
        "    symbol = \"AAPL\"  # Replace with your desired stock symbol\n",
        "    count = 10       # Number of messages to fetch\n",
        "\n",
        "    # GPU / CPU device setting\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1. Fetch messages\n",
        "    messages = fetch_stocktwits(symbol, count=count)\n",
        "\n",
        "    if not messages:\n",
        "        print(\"No messages were retrieved.\")\n",
        "        return\n",
        "\n",
        "    # 2. Prepare dataframe structure\n",
        "    #    We'll add columns for each model's fine-tuned and untuned predictions\n",
        "    columns = [\"id\", \"text\", \"created_at\"]\n",
        "    df = pd.DataFrame(messages, columns=columns)\n",
        "\n",
        "    # 3. For each model in MODEL_CONFIGS, get predictions for:\n",
        "    #    (a) The untuned version\n",
        "    #    (b) The fine-tuned version\n",
        "    #    Then store these columns in the DataFrame.\n",
        "    for key, cfg in MODEL_CONFIGS.items():\n",
        "        base_id = cfg[\"base_id\"]\n",
        "        finetuned_path = cfg[\"finetuned_path\"]\n",
        "        finetuned_type = cfg[\"finetuned_type\"]\n",
        "        untuned_label_map = cfg[\"untuned_label_map\"]\n",
        "        finetuned_label_map = cfg[\"finetuned_label_map\"]\n",
        "\n",
        "        # -- (a) Load Untuned Model --\n",
        "        print(f\"\\nLoading untuned model: {base_id}\")\n",
        "        untuned_tokenizer, untuned_model = load_model_and_tokenizer(\n",
        "            base_id=base_id,\n",
        "            use_finetuned=False,\n",
        "            device=device\n",
        "        )\n",
        "        # Predict\n",
        "        print(f\"Predicting sentiments with untuned model: {key}\")\n",
        "        untuned_preds = [\n",
        "            predict_sentiment(\n",
        "                msg[\"text\"], untuned_tokenizer, untuned_model, untuned_label_map, device=device\n",
        "            )\n",
        "            for msg in messages\n",
        "        ]\n",
        "        df[f\"untuned_{key}\"] = untuned_preds\n",
        "\n",
        "        # -- (b) Load Fine-Tuned Model --\n",
        "        print(f\"Loading fine-tuned model: {finetuned_path}\")\n",
        "        ft_tokenizer, ft_model = load_model_and_tokenizer(\n",
        "            base_id=base_id,\n",
        "            finetuned_path=finetuned_path,\n",
        "            finetuned_type=finetuned_type,\n",
        "            use_finetuned=True,\n",
        "            device=device\n",
        "        )\n",
        "        # Predict\n",
        "        print(f\"Predicting sentiments with fine-tuned model: {key}\")\n",
        "        finetuned_preds = [\n",
        "            predict_sentiment(\n",
        "                msg[\"text\"], ft_tokenizer, ft_model, finetuned_label_map, device=device\n",
        "            )\n",
        "            for msg in messages\n",
        "        ]\n",
        "        df[f\"finetuned_{key}\"] = finetuned_preds\n",
        "\n",
        "    # 4. Save results to CSV\n",
        "    csv_filename = f\"stocktwits_{symbol}_{count}_messages.csv\"\n",
        "    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')  # utf-8-sig for better compatibility\n",
        "    print(f\"\\nPredictions saved to {csv_filename}\")\n",
        "\n",
        "    # 5. (Optional) Preview CSV content\n",
        "    print(\"\\nFile content preview:\")\n",
        "    print(df.head())  # Display first few rows instead of the entire file\n",
        "\n",
        "    # 6. (Optional) Download the CSV file to your local machine (Colab)\n",
        "    try:\n",
        "        files.download(csv_filename)\n",
        "    except:\n",
        "        print(\"Download functionality is not available in this environment.\")\n",
        "\n",
        "###############################################################################\n",
        "#                                RUN SCRIPT                                   #\n",
        "###############################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hH_s8XrBYZk9",
        "outputId": "b98d85b3-18b1-46c2-e4ce-fa1ae920bd3d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Loading untuned model: prajjwal1/bert-tiny\n",
            "Loading untuned model from prajjwal1/bert-tiny\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting sentiments with untuned model: prajjwal1-bert-tiny\n",
            "Loading fine-tuned model: /content/prajjwal1_bert_tiny_model.safetensors\n",
            "Loading fine-tuned model from /content/prajjwal1_bert_tiny_model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting sentiments with fine-tuned model: prajjwal1-bert-tiny\n",
            "\n",
            "Loading untuned model: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
            "Loading untuned model from cardiffnlp/twitter-roberta-base-sentiment-latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting sentiments with untuned model: cardiffnlp-twitter-roberta-base\n",
            "Loading fine-tuned model: /content/roberta_model.pt\n",
            "Loading fine-tuned model from /content/roberta_model.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<ipython-input-8-1e8bad1a247a>:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(finetuned_path, map_location=torch.device(device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting sentiments with fine-tuned model: cardiffnlp-twitter-roberta-base\n",
            "\n",
            "Loading untuned model: Jedida/tweet_sentiments_analysis_distilbert\n",
            "Loading untuned model from Jedida/tweet_sentiments_analysis_distilbert\n",
            "Predicting sentiments with untuned model: Jedida-distilbert\n",
            "Loading fine-tuned model: /content/Jedida_tweet_sentiments_analysis_distilbert_model.safetensors\n",
            "Loading fine-tuned model from /content/Jedida_tweet_sentiments_analysis_distilbert_model.safetensors\n",
            "Predicting sentiments with fine-tuned model: Jedida-distilbert\n",
            "\n",
            "Predictions saved to stocktwits_AAPL_10_messages.csv\n",
            "\n",
            "File content preview:\n",
            "          id                                               text  \\\n",
            "0  600611392             $AAPL $SPY $QQQ let the fun begin.....   \n",
            "1  600611197  $AAPL Apple Stock Prediction 2025, 2026, 2030 ...   \n",
            "2  600610880  $AAPL no they are not, and plton is not the ne...   \n",
            "3  600610576  $AAPL $SPY $QQQ Biden&#39;s last CPA with fake...   \n",
            "4  600610535  $AAPL ðŸŽ¯ðŸ’¯ðŸ‘‡Since alert - it is oversold now. Cou...   \n",
            "\n",
            "             created_at untuned_prajjwal1-bert-tiny  \\\n",
            "0  2025-01-18T17:03:06Z                     Neutral   \n",
            "1  2025-01-18T16:59:54Z                     Neutral   \n",
            "2  2025-01-18T16:54:15Z                     Neutral   \n",
            "3  2025-01-18T16:49:08Z                     Neutral   \n",
            "4  2025-01-18T16:48:24Z                     Neutral   \n",
            "\n",
            "  finetuned_prajjwal1-bert-tiny untuned_cardiffnlp-twitter-roberta-base  \\\n",
            "0                       Neutral                                Positive   \n",
            "1                       Neutral                                 Neutral   \n",
            "2                       Bearish                                Negative   \n",
            "3                       Bearish                                Negative   \n",
            "4                       Bullish                                 Neutral   \n",
            "\n",
            "  finetuned_cardiffnlp-twitter-roberta-base untuned_Jedida-distilbert  \\\n",
            "0                                   Neutral                   Neutral   \n",
            "1                                   Neutral                   Neutral   \n",
            "2                                   Bearish                   Neutral   \n",
            "3                                   Bearish                   Neutral   \n",
            "4                                   Bullish                   Neutral   \n",
            "\n",
            "  finetuned_Jedida-distilbert  \n",
            "0                     Neutral  \n",
            "1                     Neutral  \n",
            "2                     Bearish  \n",
            "3                     Bearish  \n",
            "4                     Bullish  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d55e6176-ed8d-42f7-a912-e7b45f72eca3\", \"stocktwits_AAPL_10_messages.csv\", 2393)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}